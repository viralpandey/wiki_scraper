{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPOS Data Engineer Intern Assignment by Viral Pandey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASSIGNMENT :  Write a scraper in either python or NodeJS to collect data from Wikipedia about the top cities in the United States. The fields you collect, as well as the volume of data is up to you, but ideally you add additional data beyond the initial table, such as data found on the individual city pages, or other sources of your choice. The final format should be a CSV file that is ready to be uploaded to a BigQuery table. Please read Bigqueryâ€™s Manual to prepare your CSV in the right format. Intermediary steps, environments or processes necessary to run the scraper should be documented in code as well as a Readme.md and hosted on github in a repo devoted to this assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Python Notebook for this assignment where I have used Beautiful Soup to scrape the List of top US cities by population from Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /anaconda3/lib/python3.7/site-packages (0.23.4)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /anaconda3/lib/python3.7/site-packages (from pandas) (2.7.5)\n",
      "Requirement already satisfied: pytz>=2011k in /anaconda3/lib/python3.7/site-packages (from pandas) (2018.7)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /anaconda3/lib/python3.7/site-packages (from pandas) (1.15.4)\n",
      "Requirement already satisfied: six>=1.5 in /anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
      "Requirement already satisfied: numpy in /anaconda3/lib/python3.7/site-packages (1.15.4)\n",
      "Requirement already satisfied: beautifulsoup4 in /anaconda3/lib/python3.7/site-packages (4.7.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /anaconda3/lib/python3.7/site-packages (from beautifulsoup4) (1.9.1)\n",
      "Requirement already satisfied: requests in /anaconda3/lib/python3.7/site-packages (2.21.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /anaconda3/lib/python3.7/site-packages (from requests) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /anaconda3/lib/python3.7/site-packages (from requests) (1.24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.7/site-packages (from requests) (2018.11.29)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.7/site-packages (from requests) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "## installing necessary packages for this assignement if not present in the system\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install beautifulsoup4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Following packages are needed for this assignment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function scrapes data from the main table of top cities of US by population \n",
    "from Wikipedia. \n",
    "It returns a pandas DataFrame\n",
    "'''\n",
    "def get_main_table():\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\"\n",
    "    list_b = BeautifulSoup(requests.get(url).text, features=\"lxml\")\n",
    "    wiki_tables = list_b.find_all(\"table\", {\"class\": \"wikitable\"})\n",
    "    citylist_table = wiki_tables[1]\n",
    "    list_rows = citylist_table.find(\"tbody\").find_all(\"tr\")[1:]\n",
    "    list_headers = [\"Rank\", \"City\", \"State\", \"2018 Estimate\", \"2010 Census\", \n",
    "                         \"Change\", \"2016 Land Area Km\", \"2016 Land Area Mi\", \n",
    "                         \"2016 Population Density km\", \"2016 Population Density mi\"]\n",
    "    city_list = []\n",
    "\n",
    "    for i, city in enumerate(list_rows):\n",
    "        cities_data = city.find_all(\"td\")\n",
    "        city_dict = {}\n",
    "        for index, city_data in enumerate(cities_data):\n",
    "            if index < 10:\n",
    "                if index == 1:\n",
    "                    city_data = city_data.find(\"a\")\n",
    "                    city_dict[list_headers[index]] = city_data.text.replace(\"\\n\", \"\")\n",
    "                    city_dict[\"Wiki link\"] = city_data['href']\n",
    "                else:\n",
    "                    city_dict[list_headers[index]] = city_data.text.replace(\"\\n\", \"\")\n",
    "\n",
    "        if city_dict:\n",
    "            city_list.append(city_dict)\n",
    "\n",
    "    city_df = pd.DataFrame(city_list)\n",
    "    city_df[\"Land Area\"] = np.NaN\n",
    "    city_df[\"Water Area\"] = np.NaN\n",
    "    city_df['Climate'] = np.NaN\n",
    "    city_df['Mayor'] = np.NaN\n",
    "    city_df['Time Zone'] = np.NaN\n",
    "    city_df['Airport'] = np.NaN\n",
    "    city_df['Official Website'] = np.NaN\n",
    "\n",
    "    return city_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function scrapes data from the individual cities mentioned in the main tablelist \n",
    "and scrapes additional information of it by going through the its wiki link. \n",
    "It takes input of a city as a pandas series object, adds additional information and then \n",
    "returns it's pandas series object\n",
    "'''\n",
    "def get_city_data(city):\n",
    "    city_b = BeautifulSoup(requests.get(\"https://en.wikipedia.org{}\".format(city[\"Wiki link\"])).text, \n",
    "                              features=\"lxml\")\n",
    "    gen_info = city_b.find(\"table\", {\"class\": \"infobox geography vcard\"})\n",
    "    \n",
    "    #Time Zone\n",
    "    time_zone_data = gen_info.find(\"a\", text = \"Time zone\")\n",
    "    time_zone = time_zone_data.find_next(\"td\").text if time_zone_data else None\n",
    "    if time_zone:\n",
    "        city[\"Time Zone\"] = time_zone\n",
    "    \n",
    "    #Official Website\n",
    "    official_web_data = gen_info.find(\"th\", text=\"Website\")\n",
    "    official_web = official_web_data.find_next(\"a\", attrs={'href': re.compile(\"^http://\")}) if official_web_data else None\n",
    "    official_web = official_web.get('href') if official_web_data else None\n",
    "    if official_web:\n",
    "        city[\"Official Website\"] = official_web\n",
    "    \n",
    "    #Land Area    \n",
    "    land_area_data = gen_info.find(\"a\", text=\"Land\")\n",
    "    if not land_area_data:\n",
    "        try:\n",
    "            land_area_data = gen_info.find_all(\"th\", string=re.compile(\"Land\"))[0]\n",
    "        except:\n",
    "            land_area_data = None\n",
    "    land_area = land_area_data.find_next(\"td\").text if land_area_data else None\n",
    "    if land_area:\n",
    "        city[\"Land Area\"] = land_area\n",
    "        \n",
    "    # Water Area\n",
    "    water_area_data = gen_info.find(\"a\", text=\"Water\")\n",
    "    if not water_area_data:\n",
    "        try:\n",
    "            water_area_data = gen_info.find_all(\"th\", string=re.compile(\"Water\"))[0]\n",
    "        except:\n",
    "            water_area_data = None\n",
    "    water_area = water_area_data.find_next(\"td\").text if water_area_data else None\n",
    "    if water_area:\n",
    "        city[\"Water Area\"] = water_area\n",
    "\n",
    "    #Mayor\n",
    "    mayor_data = gen_info.find(\"a\", text=\"Mayor\")\n",
    "    if not mayor_data:\n",
    "        try:\n",
    "            mayor_data = gen_info.find_all(\"th\", string=re.compile(\"Mayor\"))[0]\n",
    "        except:\n",
    "            mayor_data = None\n",
    "    mayor = mayor_data.find_next(\"a\").text if mayor_data else None\n",
    "    if mayor:\n",
    "        city[\"Mayor\"] = mayor\n",
    "    \n",
    "    #Climate\n",
    "    climate_data = city_b.find('span', id='Climate')\n",
    "    if not climate_data:\n",
    "        climate_data = city_b.find('span', id=\"Weather\")        \n",
    "    if climate_data:\n",
    "        climate_data = climate_data.parent.find_next(\"p\").text\n",
    "        if climate_data:\n",
    "            city[\"Climate\"] = climate_data.replace(\"\\xa0\", \"\")\n",
    "    \n",
    "    #Airports\n",
    "    airport_data = city_b.find('span', id='Airports')\n",
    "    if not airport_data:\n",
    "        airport_data = city_b.find('span', id=\"Air\")            \n",
    "    if airport_data:\n",
    "        airport = airport_data.parent.find_next(\"p\")\n",
    "        \n",
    "        if airport:\n",
    "            airport = airport.text\n",
    "            city[\"Airport\"] = airport.replace(\"\\xa0\", \"\")\n",
    "\n",
    "    return city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is a main function that first scrapes the main table from the wikipedia link. \n",
    "Then it runs a for loop and fills extra details\n",
    "It returns a final pandas DataFrame required by this assignment\n",
    "'''\n",
    "def top_cities_US():\n",
    "    city_df = get_main_table()\n",
    "\n",
    "    for i, city in city_df.iterrows():\n",
    "        city = get_city_data(city)\n",
    "        city_df.loc[i] = city\n",
    "\n",
    "    return city_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cities_US_df = top_cities_US()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Rearrange the columns according to the table on the Wikipedia Page\n",
    "'''\n",
    "cols = top_cities_US_df.columns.tolist()\n",
    "cols = [\"Rank\", \"City\", \"State\", \"2018 Estimate\", \"2010 Census\", \n",
    "        \"Change\", \"2016 Land Area Km\", \"2016 Land Area Mi\", \n",
    "        \"2016 Population Density km\", \"2016 Population Density mi\", \n",
    "        \"Land Area\", \"Water Area\", \"Time Zone\", \"Mayor\",\"Airport\",\n",
    "        \"Official Website\", \"Climate\", \"Wiki link\"]\n",
    "top_cities_US_df = top_cities_US_df[cols] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Convert the Pandas DataFrame into a csv file and write it to the local machine\n",
    "'''\n",
    "f = open(\"top_cities_US.csv\", \"w\")\n",
    "f.write(top_cities_US_df.to_csv(index = False))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
